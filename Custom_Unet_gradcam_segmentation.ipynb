{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5de4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Full training + interpretability (Grad-CAM) with FIXED scaling\n",
    "# Works for 6-band Landsat-style inputs where values may be 0..10000\n",
    "# Band order assumed: [R, G, B, NIR, SWIR1, SWIR2]\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, MaxPooling2D, Conv2DTranspose,\n",
    "    Concatenate, BatchNormalization, Activation\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "\n",
    "# =========================\n",
    "# Parameters\n",
    "# =========================\n",
    "tile_size = 256\n",
    "input_channels = 6\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "thr = 0.5\n",
    "\n",
    "OUT_DIR = \"reviewer_comment10_outputs_6band_fixed\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# Load your data\n",
    "# =========================\n",
    "# tiles_img.shape = (num_samples, 256, 256, 6)\n",
    "# tiles_mask.shape = (num_samples, 256, 256)\n",
    "tiles_img, tiles_mask = load_data(\"datasets_temp/images\", \"datasets_temp/masks\", tile_size)\n",
    "\n",
    "tiles_mask = tiles_mask.reshape(tiles_mask.shape + (1,)).astype(np.uint8)\n",
    "\n",
    "# =========================\n",
    "# Sanity checks (critical)\n",
    "# =========================\n",
    "print(\"tiles_img dtype:\", tiles_img.dtype)\n",
    "print(\"tiles_img global min/max:\", float(np.min(tiles_img)), float(np.max(tiles_img)))\n",
    "print(\"tiles_mask unique:\", np.unique(tiles_mask))\n",
    "\n",
    "for k in range(min(input_channels, tiles_img.shape[-1])):\n",
    "    v = tiles_img[..., k]\n",
    "    print(f\"band[{k}] min/max/mean/std:\",\n",
    "          float(v.min()), float(v.max()), float(v.mean()), float(v.std()))\n",
    "\n",
    "# ============================================================\n",
    "# Scaling utilities (FIX)\n",
    "# ============================================================\n",
    "def scale_np_image(img):\n",
    "    \"\"\"\n",
    "    Returns float32 image approximately in [0,1] range without clipping.\n",
    "    Heuristic:\n",
    "      - if max <= 1.5: already 0..1\n",
    "      - elif max <= 300: treat as 0..255-ish -> /255\n",
    "      - else: treat as reflectance 0..10000-ish -> /10000\n",
    "    \"\"\"\n",
    "    img = img.astype(np.float32)\n",
    "    mx = float(np.max(img))\n",
    "    if mx <= 1.5:\n",
    "        return img\n",
    "    if mx <= 300.0:\n",
    "        return img / 255.0\n",
    "    return img / 10000.0\n",
    "\n",
    "@tf.function\n",
    "def scale_tf_image(image):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    mx = tf.reduce_max(image)\n",
    "\n",
    "    # reflectance-like (0..10000)\n",
    "    image = tf.cond(mx > 300.0, lambda: image / 10000.0, lambda: image)\n",
    "    # 8-bit-like (0..255)\n",
    "    image = tf.cond((mx > 1.5) & (mx <= 300.0), lambda: image / 255.0, lambda: image)\n",
    "    return image\n",
    "\n",
    "# ============================================================\n",
    "# Visualization helpers (FIX)\n",
    "# ============================================================\n",
    "def stretch_rgb_per_channel(rgb, p_low=2, p_high=98, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Robust per-channel percentile stretch for visualization.\n",
    "    Handles near-constant channels safely.\n",
    "    \"\"\"\n",
    "    rgb = rgb.astype(np.float32)\n",
    "    out = np.zeros_like(rgb, dtype=np.float32)\n",
    "    for c in range(3):\n",
    "        lo = np.percentile(rgb[..., c], p_low)\n",
    "        hi = np.percentile(rgb[..., c], p_high)\n",
    "        if abs(hi - lo) < eps:\n",
    "            out[..., c] = rgb[..., c]\n",
    "        else:\n",
    "            out[..., c] = (rgb[..., c] - lo) / (hi - lo)\n",
    "    return np.clip(out, 0.0, 1.0)\n",
    "\n",
    "def safe_index(num, den, eps=1e-6):\n",
    "    den2 = np.where(np.abs(den) < eps, np.nan, den)\n",
    "    x = num / den2\n",
    "    return np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "\n",
    "def compute_indices(img01):\n",
    "    \"\"\"\n",
    "    img01 expected scaled ~0..1 (not clipped).\n",
    "    \"\"\"\n",
    "    r = img01[..., 0]\n",
    "    g = img01[..., 1]\n",
    "    b = img01[..., 2]\n",
    "    nir = img01[..., 3]\n",
    "    swir1 = img01[..., 4]\n",
    "    swir2 = img01[..., 5]\n",
    "\n",
    "    ndvi  = safe_index(nir - r, nir + r)\n",
    "    mndwi = safe_index(g - swir1, g + swir1)\n",
    "    nbr   = safe_index(nir - swir2, nir + swir2)\n",
    "    return ndvi, mndwi, nbr\n",
    "\n",
    "def binarize(p, thr=0.5):\n",
    "    return (p >= thr).astype(np.uint8)\n",
    "\n",
    "def error_masks(gt, pred):\n",
    "    fp = ((gt == 0) & (pred == 1)).astype(np.uint8)\n",
    "    fn = ((gt == 1) & (pred == 0)).astype(np.uint8)\n",
    "    tp = ((gt == 1) & (pred == 1)).astype(np.uint8)\n",
    "    tn = ((gt == 0) & (pred == 0)).astype(np.uint8)\n",
    "    return tn, fp, fn, tp\n",
    "\n",
    "def error_map(gt, pred):\n",
    "    # 0 TN, 1 FP, 2 FN, 3 TP\n",
    "    tn, fp, fn, tp = error_masks(gt, pred)\n",
    "    out = np.zeros_like(gt, dtype=np.uint8)\n",
    "    out[fp == 1] = 1\n",
    "    out[fn == 1] = 2\n",
    "    out[tp == 1] = 3\n",
    "    return out\n",
    "\n",
    "def compute_iou(gt, pred):\n",
    "    gt = gt.reshape(-1).astype(np.uint8)\n",
    "    pred = pred.reshape(-1).astype(np.uint8)\n",
    "    inter = np.sum((gt == 1) & (pred == 1))\n",
    "    union = np.sum((gt == 1) | (pred == 1))\n",
    "    return float(inter / (union + 1e-12))\n",
    "\n",
    "def pick_mixed_tiles(y_true, min_frac=0.05, max_frac=0.95):\n",
    "    frac = y_true.reshape(y_true.shape[0], -1).mean(axis=1)\n",
    "    return np.where((frac >= min_frac) & (frac <= max_frac))[0]\n",
    "\n",
    "# ============================================================\n",
    "# Model: custom 6-band U-Net\n",
    "# ============================================================\n",
    "def custom_unet(input_size=(tile_size, tile_size, input_channels)):\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    # Encoder\n",
    "    c1 = Conv2D(64, (3, 3), padding='same')(inputs)\n",
    "    c1 = BatchNormalization()(c1)\n",
    "    c1 = Activation(\"relu\")(c1)\n",
    "    c1 = Conv2D(64, (3, 3), padding='same')(c1)\n",
    "    c1 = Activation(\"relu\")(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = Conv2D(128, (3, 3), padding='same')(p1)\n",
    "    c2 = BatchNormalization()(c2)\n",
    "    c2 = Activation(\"relu\")(c2)\n",
    "    c2 = Conv2D(128, (3, 3), padding='same')(c2)\n",
    "    c2 = Activation(\"relu\")(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = Conv2D(256, (3, 3), padding='same')(p2)\n",
    "    c3 = BatchNormalization()(c3)\n",
    "    c3 = Activation(\"relu\")(c3)\n",
    "    c3 = Conv2D(256, (3, 3), padding='same')(c3)\n",
    "    c3 = Activation(\"relu\")(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = Conv2D(512, (3, 3), padding='same')(p3)\n",
    "    c4 = BatchNormalization()(c4)\n",
    "    c4 = Activation(\"relu\")(c4)\n",
    "    c4 = Conv2D(512, (3, 3), padding='same')(c4)\n",
    "    c4 = Activation(\"relu\")(c4)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "    # Bottleneck\n",
    "    c5 = Conv2D(1024, (3, 3), padding='same')(p4)\n",
    "    c5 = BatchNormalization()(c5)\n",
    "    c5 = Activation(\"relu\")(c5)\n",
    "    c5 = Conv2D(1024, (3, 3), padding='same')(c5)\n",
    "    c5 = Activation(\"relu\")(c5)\n",
    "\n",
    "    # Decoder\n",
    "    u6 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = Concatenate()([u6, c4])\n",
    "    c6 = Conv2D(512, (3, 3), padding='same')(u6)\n",
    "    c6 = BatchNormalization()(c6)\n",
    "    c6 = Activation(\"relu\")(c6)\n",
    "    c6 = Conv2D(512, (3, 3), padding='same')(c6)\n",
    "    c6 = Activation(\"relu\")(c6)\n",
    "\n",
    "    u7 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = Concatenate()([u7, c3])\n",
    "    c7 = Conv2D(256, (3, 3), padding='same')(u7)\n",
    "    c7 = BatchNormalization()(c7)\n",
    "    c7 = Activation(\"relu\")(c7)\n",
    "    c7 = Conv2D(256, (3, 3), padding='same')(c7)\n",
    "    c7 = Activation(\"relu\")(c7)\n",
    "\n",
    "    u8 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = Concatenate()([u8, c2])\n",
    "    c8 = Conv2D(128, (3, 3), padding='same')(u8)\n",
    "    c8 = BatchNormalization()(c8)\n",
    "    c8 = Activation(\"relu\")(c8)\n",
    "    c8 = Conv2D(128, (3, 3), padding='same')(c8)\n",
    "    c8 = Activation(\"relu\")(c8)\n",
    "\n",
    "    u9 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = Concatenate()([u9, c1])\n",
    "    c9 = Conv2D(64, (3, 3), padding='same', name=\"last_decoder_conv\")(u9)\n",
    "    c9 = BatchNormalization()(c9)\n",
    "    c9 = Activation(\"relu\")(c9)\n",
    "    c9 = Conv2D(64, (3, 3), padding='same')(c9)\n",
    "    c9 = Activation(\"relu\")(c9)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid', name=\"segmentation_head\")(c9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "unet_model = custom_unet(input_size=(tile_size, tile_size, input_channels))\n",
    "unet_model.summary()\n",
    "\n",
    "# =========================\n",
    "# LR scheduler (fixed to decay progressively)\n",
    "# =========================\n",
    "def lr_schedule(epoch, base=1e-4, decay=0.9, step=10):\n",
    "    n = epoch // step\n",
    "    return base * (decay ** n)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# =========================\n",
    "# TF Dataset with augmentation (FIXED scaling)\n",
    "# =========================\n",
    "def preprocess_tf(image, mask):\n",
    "    image = scale_tf_image(image)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    return image, mask\n",
    "\n",
    "def augment_tf(image, mask):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    mask  = tf.image.random_flip_left_right(mask)\n",
    "    mask  = tf.image.random_flip_up_down(mask)\n",
    "\n",
    "    k = tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)\n",
    "    image = tf.image.rot90(image, k)\n",
    "    mask  = tf.image.rot90(mask, k)\n",
    "    return image, mask\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tiles_img, tiles_mask, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.map(preprocess_tf, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.map(augment_tf, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(200).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset = val_dataset.map(preprocess_tf, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# =========================\n",
    "# Callbacks\n",
    "# =========================\n",
    "checkpointer = ModelCheckpoint(\"best_weight_unet_6band_fixedscale.h5\",\n",
    "                               monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "earlyStopping = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=1, mode=\"min\")\n",
    "callbacks = [lr_scheduler, earlyStopping, checkpointer]\n",
    "\n",
    "# =========================\n",
    "# Train\n",
    "# =========================\n",
    "history = unet_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Evaluate\n",
    "# =========================\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.map(preprocess_tf, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_loss, test_accuracy = unet_model.evaluate(test_dataset, verbose=1)\n",
    "print(\"Test Loss:\", float(test_loss), \"Test Accuracy:\", float(test_accuracy))\n",
    "\n",
    "# =========================\n",
    "# Save model\n",
    "# =========================\n",
    "unet_model.save(\"forest_detection_model_6band_unet_fixedscale.h5\")\n",
    "\n",
    "# =========================\n",
    "# Plot training history\n",
    "# =========================\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# Grad-CAM for segmentation (FIXED scaling + better visuals)\n",
    "# ============================================================\n",
    "def auto_pick_cam_layer(model):\n",
    "    if \"last_decoder_conv\" in [l.name for l in model.layers]:\n",
    "        return \"last_decoder_conv\"\n",
    "    convs = [l for l in model.layers if isinstance(l, tf.keras.layers.Conv2D)]\n",
    "    for l in reversed(convs):\n",
    "        k = getattr(l, \"kernel_size\", (1, 1))\n",
    "        if k[0] > 1 or k[1] > 1:\n",
    "            return l.name\n",
    "    return convs[-1].name\n",
    "\n",
    "def gradcam_segmentation(model, img01, cam_layer_name, region_mask=None):\n",
    "    \"\"\"\n",
    "    img01: (H,W,C) float32 scaled ~0..1\n",
    "    region_mask: (H,W) float {0,1} to focus gradients on GT/FP/FN pixels\n",
    "    \"\"\"\n",
    "    img = tf.convert_to_tensor(img01[None, ...], dtype=tf.float32)\n",
    "\n",
    "    cam_layer = model.get_layer(cam_layer_name)\n",
    "    grad_model = tf.keras.Model([model.inputs], [cam_layer.output, model.output])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_out, pred = grad_model(img, training=False)     # pred: (1,H,W,1)\n",
    "        pred = pred[..., 0]                                  # (1,H,W)\n",
    "        if region_mask is None:\n",
    "            target = tf.reduce_mean(pred)\n",
    "        else:\n",
    "            rm = tf.convert_to_tensor(region_mask[None, ...], dtype=tf.float32)\n",
    "            target = tf.reduce_sum(pred * rm) / (tf.reduce_sum(rm) + 1e-12)\n",
    "\n",
    "    grads = tape.gradient(target, conv_out)                  # (1,h,w,c)\n",
    "    weights = tf.reduce_mean(grads, axis=(1, 2))             # (1,c)\n",
    "    cam = tf.reduce_sum(conv_out * weights[:, None, None, :], axis=-1)  # (1,h,w)\n",
    "    cam = tf.nn.relu(cam)[0].numpy()\n",
    "\n",
    "    cam = tf.image.resize(cam[..., None], (img01.shape[0], img01.shape[1]), method=\"bilinear\").numpy()[..., 0]\n",
    "    cam = cam - cam.min()\n",
    "    cam = cam / (cam.max() + 1e-12)\n",
    "    return cam.astype(np.float32)\n",
    "\n",
    "def save_gradcam_panels(X, y_true, y_prob, idxs, cam_layer, out_png, focus=\"gt\"):\n",
    "    n = len(idxs)\n",
    "    fig, axs = plt.subplots(n, 5, figsize=(14, 2.7*n))\n",
    "    if n == 1:\n",
    "        axs = np.expand_dims(axs, axis=0)\n",
    "\n",
    "    for r, idx in enumerate(idxs):\n",
    "        img01 = scale_np_image(X[idx])\n",
    "        rgb_vis = stretch_rgb_per_channel(img01[..., :3])\n",
    "\n",
    "        gt = y_true[idx].astype(np.uint8)\n",
    "        prob = y_prob[idx].astype(np.float32)\n",
    "        pred = binarize(prob, thr)\n",
    "        em = error_map(gt, pred)\n",
    "\n",
    "        tn, fp, fn, tp = error_masks(gt, pred)\n",
    "\n",
    "        if focus == \"gt\":\n",
    "            region = gt.astype(np.float32)\n",
    "        elif focus == \"fp\":\n",
    "            region = fp.astype(np.float32)\n",
    "        elif focus == \"fn\":\n",
    "            region = fn.astype(np.float32)\n",
    "        else:\n",
    "            region = None\n",
    "\n",
    "        cam = gradcam_segmentation(unet_model, img01, cam_layer, region_mask=region)\n",
    "\n",
    "        axs[r, 0].imshow(rgb_vis); axs[r, 0].set_title(f\"RGB (tile {idx})\")\n",
    "        axs[r, 1].imshow(gt, vmin=0, vmax=1); axs[r, 1].set_title(\"Ground truth\")\n",
    "        axs[r, 2].imshow(pred, vmin=0, vmax=1); axs[r, 2].set_title(\"Prediction\")\n",
    "        axs[r, 3].imshow(em, vmin=0, vmax=3); axs[r, 3].set_title(\"Errors (FP/FN/TP)\")\n",
    "        axs[r, 4].imshow(rgb_vis)\n",
    "        axs[r, 4].imshow(cam, alpha=0.45)\n",
    "        axs[r, 4].set_title(f\"Grad-CAM ({focus})\")\n",
    "\n",
    "        for c in range(5):\n",
    "            axs[r, c].set_xticks([]); axs[r, c].set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def save_indices_panel(X, idxs, out_png):\n",
    "    n = len(idxs)\n",
    "    fig, axs = plt.subplots(n, 4, figsize=(12, 2.7*n))\n",
    "    if n == 1:\n",
    "        axs = np.expand_dims(axs, axis=0)\n",
    "\n",
    "    for r, idx in enumerate(idxs):\n",
    "        img01 = scale_np_image(X[idx])\n",
    "        rgb_vis = stretch_rgb_per_channel(img01[..., :3])\n",
    "        ndvi, mndwi, nbr = compute_indices(img01)\n",
    "\n",
    "        axs[r, 0].imshow(rgb_vis); axs[r, 0].set_title(f\"RGB (tile {idx})\")\n",
    "        axs[r, 1].imshow(ndvi, vmin=-1, vmax=1); axs[r, 1].set_title(\"NDVI\")\n",
    "        axs[r, 2].imshow(mndwi, vmin=-1, vmax=1); axs[r, 2].set_title(\"MNDWI\")\n",
    "        axs[r, 3].imshow(nbr, vmin=-1, vmax=1); axs[r, 3].set_title(\"NBR\")\n",
    "\n",
    "        for c in range(4):\n",
    "            axs[r, c].set_xticks([]); axs[r, c].set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# Predict on test set (IMPORTANT: scaled correctly)\n",
    "# ============================================================\n",
    "X01_test = np.stack([scale_np_image(x) for x in X_test], axis=0).astype(np.float32)\n",
    "y_true = y_test[..., 0].astype(np.uint8)\n",
    "\n",
    "y_prob = unet_model.predict(X01_test, batch_size=16, verbose=1)[..., 0]  # (N,H,W)\n",
    "\n",
    "# pick mixed tiles so panels are meaningful\n",
    "mixed = pick_mixed_tiles(y_true, min_frac=0.05, max_frac=0.95)\n",
    "if mixed.size == 0:\n",
    "    mixed = pick_mixed_tiles(y_true, min_frac=0.01, max_frac=0.99)\n",
    "\n",
    "ious = []\n",
    "for idx in mixed:\n",
    "    pred = binarize(y_prob[idx], thr)\n",
    "    ious.append(compute_iou(y_true[idx], pred))\n",
    "ious = np.array(ious, dtype=np.float32)\n",
    "\n",
    "N_SHOW = 6\n",
    "best_idxs  = mixed[np.argsort(-ious)[:N_SHOW]]\n",
    "worst_idxs = mixed[np.argsort(ious)[:N_SHOW]]\n",
    "\n",
    "cam_layer = auto_pick_cam_layer(unet_model)\n",
    "print(\"Grad-CAM layer:\", cam_layer)\n",
    "print(\"Best idx:\", best_idxs)\n",
    "print(\"Worst idx:\", worst_idxs)\n",
    "\n",
    "save_gradcam_panels(X_test, y_true, y_prob, best_idxs,  cam_layer,\n",
    "                    os.path.join(OUT_DIR, \"gradcam_best_mixed_gtfocus.png\"), focus=\"gt\")\n",
    "\n",
    "save_gradcam_panels(X_test, y_true, y_prob, worst_idxs, cam_layer,\n",
    "                    os.path.join(OUT_DIR, \"gradcam_worst_mixed_gtfocus.png\"), focus=\"gt\")\n",
    "\n",
    "save_gradcam_panels(X_test, y_true, y_prob, worst_idxs, cam_layer,\n",
    "                    os.path.join(OUT_DIR, \"gradcam_worst_mixed_fp_focus.png\"), focus=\"fp\")\n",
    "\n",
    "save_gradcam_panels(X_test, y_true, y_prob, worst_idxs, cam_layer,\n",
    "                    os.path.join(OUT_DIR, \"gradcam_worst_mixed_fn_focus.png\"), focus=\"fn\")\n",
    "\n",
    "save_indices_panel(X_test, worst_idxs, os.path.join(OUT_DIR, \"spectral_indices_worst_mixed.png\"))\n",
    "\n",
    "# ============================================================\n",
    "# Simple per-tile proxies + stratified summary (optional but helpful)\n",
    "# ============================================================\n",
    "def tile_proxies(img01):\n",
    "    rgb = img01[..., :3]\n",
    "    brightness = rgb.mean(axis=-1)\n",
    "\n",
    "    # \"cloud/haze-like\" and \"shadow-like\" are just simple proxies\n",
    "    bright_frac = float(np.mean(brightness > 0.85))\n",
    "    dark_frac   = float(np.mean(brightness < 0.15))\n",
    "\n",
    "    ndvi, mndwi, nbr = compute_indices(img01)\n",
    "    water_frac = float(np.mean(mndwi > 0.2))   # water/inundation proxy\n",
    "    veg_frac   = float(np.mean(ndvi > 0.4))    # strong vegetation proxy\n",
    "\n",
    "    return {\n",
    "        \"mean_brightness\": float(brightness.mean()),\n",
    "        \"std_brightness\": float(brightness.std()),\n",
    "        \"bright_frac\": bright_frac,\n",
    "        \"dark_frac\": dark_frac,\n",
    "        \"water_frac_mndwi\": water_frac,\n",
    "        \"veg_frac_ndvi\": veg_frac,\n",
    "        \"ndvi_mean\": float(ndvi.mean()),\n",
    "        \"mndwi_mean\": float(mndwi.mean()),\n",
    "        \"nbr_mean\": float(nbr.mean()),\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    img01 = X01_test[i]\n",
    "    gt = y_true[i]\n",
    "    pred = binarize(y_prob[i], thr)\n",
    "\n",
    "    tn, fp, fn, tp = error_masks(gt, pred)\n",
    "    fp_rate = float(fp.sum() / (gt.size + 1e-12))\n",
    "    fn_rate = float(fn.sum() / (gt.size + 1e-12))\n",
    "    iou = compute_iou(gt, pred)\n",
    "\n",
    "    prox = tile_proxies(img01)\n",
    "\n",
    "    rows.append({\n",
    "        \"tile_id\": i,\n",
    "        \"gt_forest_frac\": float(gt.mean()),\n",
    "        \"iou\": iou,\n",
    "        \"fp_rate\": fp_rate,\n",
    "        \"fn_rate\": fn_rate,\n",
    "        **prox\n",
    "    })\n",
    "\n",
    "csv_path = os.path.join(OUT_DIR, \"per_tile_error_proxies_6band_fixed.csv\")\n",
    "with open(csv_path, \"w\", newline=\"\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "    w.writeheader()\n",
    "    w.writerows(rows)\n",
    "\n",
    "def summarize_group(name, selector):\n",
    "    sel = np.array([selector(r) for r in rows], dtype=bool)\n",
    "    if sel.sum() == 0:\n",
    "        return None\n",
    "    return {\n",
    "        \"group\": name,\n",
    "        \"n_tiles\": int(sel.sum()),\n",
    "        \"mean_fp_rate\": float(np.mean([rows[i][\"fp_rate\"] for i in range(len(rows)) if sel[i]])),\n",
    "        \"mean_fn_rate\": float(np.mean([rows[i][\"fn_rate\"] for i in range(len(rows)) if sel[i]])),\n",
    "        \"mean_iou\": float(np.mean([rows[i][\"iou\"] for i in range(len(rows)) if sel[i]])),\n",
    "    }\n",
    "\n",
    "groups = []\n",
    "groups.append(summarize_group(\"high_cloud_haze (bright_frac>0.10)\", lambda r: r[\"bright_frac\"] > 0.10))\n",
    "groups.append(summarize_group(\"high_shadow (dark_frac>0.10)\", lambda r: r[\"dark_frac\"] > 0.10))\n",
    "groups.append(summarize_group(\"high_water (water_frac_mndwi>0.10)\", lambda r: r[\"water_frac_mndwi\"] > 0.10))\n",
    "groups.append(summarize_group(\"low_water (water_frac_mndwi<=0.10)\", lambda r: r[\"water_frac_mndwi\"] <= 0.10))\n",
    "groups = [g for g in groups if g is not None]\n",
    "\n",
    "sum_path = os.path.join(OUT_DIR, \"stratified_error_summary_6band_fixed.csv\")\n",
    "if len(groups) > 0:\n",
    "    with open(sum_path, \"w\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=list(groups[0].keys()))\n",
    "        w.writeheader()\n",
    "        w.writerows(groups)\n",
    "\n",
    "print(\"Saved outputs to:\", OUT_DIR)\n",
    "print(\"Figures:\")\n",
    "print(\" - gradcam_best_mixed_gtfocus.png\")\n",
    "print(\" - gradcam_worst_mixed_gtfocus.png\")\n",
    "print(\" - gradcam_worst_mixed_fp_focus.png\")\n",
    "print(\" - gradcam_worst_mixed_fn_focus.png\")\n",
    "print(\" - spectral_indices_worst_mixed.png\")\n",
    "print(\"CSVs:\")\n",
    "print(\" -\", csv_path)\n",
    "if len(groups) > 0:\n",
    "    print(\" -\", sum_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ba2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
